# -*- coding: utf-8 -*-
"""RP_Two_Tower_Network.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XqJsh9-86ixD0uFM8E5fLvFtukeXoW_4
"""

import pandas as pd
import numpy as np
import ast
import random

# Load interaction data (user-item interactions with implicit feedback signals)
interactions = pd.read_csv('interaction_table.csv')
# Load user metadata (user country and supported languages)
users_df = pd.read_csv('user_table_final.csv')
# Load item metadata (item country and language)
posts_df = pd.read_csv('post_table.csv')

interactions.head(5)

users_df.head(5)

posts_df.head(5)

print("Interactions shape:", interactions.shape)

print("Users Table shape:",users_df.shape)

print("Post Table shape:",posts_df.shape)

# We will use 'country' and 'primary_lang' as user features in the model.

users_df['primary_lang'] = users_df['supported_languages'].apply(
    lambda x: ast.literal_eval(x)[0] if pd.notnull(x) else None
)

users_df.head(5)

# We will use 'effective_country' as item country and 'lang' as item language features.

posts_df.rename(columns={'effective_country': 'item_country', 'lang': 'item_lang'}, inplace=True)

# We will use 'effective_country' as item country and 'lang' as item language features.

posts_df.rename(columns={'effective_country': 'item_country', 'lang': 'item_lang'}, inplace=True)

print("Unique user countries:", users_df['country'].unique())
print("Unique user primary languages:", users_df['primary_lang'].unique())
print("Unique item countries:", posts_df['item_country'].unique())
print("Unique item languages:", posts_df['item_lang'].unique())

user_to_country = pd.Series(users_df.country.values, index=users_df.user_id).to_dict()
user_to_lang    = pd.Series(users_df.primary_lang.values, index=users_df.user_id).to_dict()
item_to_country = pd.Series(posts_df.item_country.values, index=posts_df.post_id).to_dict()
item_to_lang    = pd.Series(posts_df.item_lang.values, index=posts_df.post_id).to_dict()

user_to_country

user_to_lang

item_to_country

item_to_lang

# In implicit feedback, any form of engagement (like, save, view) can be considered a positive signal.
# Here, we assume each row in interactions is a positive instance (user interacted with item).
# We will create positive samples (label=1) from these interactions.


pos_user_ids = interactions['user_id'].tolist()
pos_item_ids = interactions['post_id'].tolist()
pos_labels   = [1] * len(pos_user_ids)  # all positives labeled 1

pos_user_ids[:5]

pos_item_ids[:5]

pos_labels[:5]

# We also need negative samples (user did NOT interact with item) for training.
# We'll do negative sampling by pairing each user with some items they have not interacted with.
# For each positive interaction, we sample `neg_ratio` negative examples.

user_pos_set = interactions.groupby('user_id')['post_id'].apply(set).to_dict()  # set of items each user interacted with
all_items = posts_df['post_id'].unique().tolist()
neg_ratio = 4  # number of negative samples per positive sample

neg_user_ids = []
neg_item_ids = []
neg_labels   = []

random.seed(42)  # for reproducibility
for user, pos_items in user_pos_set.items():
    for pos_item in pos_items:
        # Generate `neg_ratio` negatives for this positive interaction
        for _ in range(neg_ratio):
            neg_item = random.choice(all_items)
            # Ensure the sampled item is not one the user has interacted with
            while neg_item in pos_items:
                neg_item = random.choice(all_items)
            neg_user_ids.append(user)
            neg_item_ids.append(neg_item)
            neg_labels.append(0)  # negative label 0

print(f"Generated {len(pos_user_ids)} positive samples and {len(neg_user_ids)} negative samples.")

# Combine positive and negative samples
all_user_ids = pos_user_ids + neg_user_ids
all_item_ids = pos_item_ids + neg_item_ids
all_labels   = pos_labels   + neg_labels

# Encode categorical IDs and features as numeric indices for model input.
# We will create index mappings for user IDs, item IDs, user countries, user languages, item countries, item languages.
user_ids_unique         = sorted(users_df['user_id'].unique().tolist())
item_ids_unique         = sorted(posts_df['post_id'].unique().tolist())
user_countries_unique   = sorted(users_df['country'].unique().tolist())
user_langs_unique       = sorted(users_df['primary_lang'].unique().tolist())
item_countries_unique   = sorted(posts_df['item_country'].unique().tolist())
item_langs_unique       = sorted(posts_df['item_lang'].unique().tolist())

user_id_to_index        = {uid: idx for idx, uid in enumerate(user_ids_unique)}
item_id_to_index        = {pid: idx for idx, pid in enumerate(item_ids_unique)}
user_country_to_index   = {c: idx for idx, c in enumerate(user_countries_unique)}
user_lang_to_index      = {l: idx for idx, l in enumerate(user_langs_unique)}
item_country_to_index   = {c: idx for idx, c in enumerate(item_countries_unique)}
item_lang_to_index      = {l: idx for idx, l in enumerate(item_langs_unique)}

# Map all user and item features to their indices
user_index_data        = [user_id_to_index[u] for u in all_user_ids]
user_country_index_data= [user_country_to_index[user_to_country[u]] for u in all_user_ids]
user_lang_index_data   = [user_lang_to_index[user_to_lang[u]] for u in all_user_ids]
item_index_data        = [item_id_to_index[i] for i in all_item_ids]
item_country_index_data= [item_country_to_index[item_to_country[i]] for i in all_item_ids]
item_lang_index_data   = [item_lang_to_index[item_to_lang[i]] for i in all_item_ids]
labels_data            = all_labels

# Convert to NumPy arrays for model training

user_index_data        = np.array(user_index_data, dtype='int32')
user_country_index_data= np.array(user_country_index_data, dtype='int32')
user_lang_index_data   = np.array(user_lang_index_data, dtype='int32')
item_index_data        = np.array(item_index_data, dtype='int32')
item_country_index_data= np.array(item_country_index_data, dtype='int32')
item_lang_index_data   = np.array(item_lang_index_data, dtype='int32')
labels_data            = np.array(labels_data, dtype='float32')

print("Sample encoded data:",
      user_index_data[0], user_country_index_data[0], user_lang_index_data[0],
      item_index_data[0], item_country_index_data[0], item_lang_index_data[0],
      "label", labels_data[0])

import tensorflow as tf
from tensorflow.keras import layers, Model

# Define input layers for the model (each input is an integer index for a feature)
user_id_input       = layers.Input(shape=(1,), dtype='int32', name='user_id')
user_country_input  = layers.Input(shape=(1,), dtype='int32', name='user_country')
user_lang_input     = layers.Input(shape=(1,), dtype='int32', name='user_lang')
item_id_input       = layers.Input(shape=(1,), dtype='int32', name='item_id')
item_country_input  = layers.Input(shape=(1,), dtype='int32', name='item_country')
item_lang_input     = layers.Input(shape=(1,), dtype='int32', name='item_lang')

# Embedding layers for each categorical feature.
# These layers convert the integer indices into dense vectors (embeddings).
num_users          = len(user_ids_unique)
num_items          = len(item_ids_unique)
num_user_countries = len(user_countries_unique)
num_user_langs     = len(user_langs_unique)
num_item_countries = len(item_countries_unique)
num_item_langs     = len(item_langs_unique)

# User tower embeddings
user_id_emb = layers.Embedding(input_dim=num_users, output_dim=32, name='user_id_emb')(user_id_input)
user_country_emb = layers.Embedding(input_dim=num_user_countries, output_dim=8, name='user_country_emb')(user_country_input)
user_lang_emb = layers.Embedding(input_dim=num_user_langs, output_dim=8, name='user_lang_emb')(user_lang_input)
# The embedding outputs have shape (batch_size, 1, embedding_dim). Flatten them to shape (batch_size, embedding_dim).
user_id_vec = layers.Flatten()(user_id_emb)
user_country_vec = layers.Flatten()(user_country_emb)
user_lang_vec = layers.Flatten()(user_lang_emb)
# Concatenate user feature vectors into a single vector for the user tower.
user_features = layers.concatenate([user_id_vec, user_country_vec, user_lang_vec], name='user_features')

# User tower: a small neural network to process user features.
# We use a Dense layer to learn interactions between user embeddings.
user_hidden = layers.Dense(32, activation='relu')(user_features)
# The output of the user tower is a user embedding vector.
user_vector = layers.Dense(32, activation=None, name='user_vector')(user_hidden)

# Item tower embeddings
item_id_emb = layers.Embedding(input_dim=num_items, output_dim=32, name='item_id_emb')(item_id_input)
item_country_emb = layers.Embedding(input_dim=num_item_countries, output_dim=8, name='item_country_emb')(item_country_input)
item_lang_emb = layers.Embedding(input_dim=num_item_langs, output_dim=8, name='item_lang_emb')(item_lang_input)
item_id_vec = layers.Flatten()(item_id_emb)
item_country_vec = layers.Flatten()(item_country_emb)
item_lang_vec = layers.Flatten()(item_lang_emb)
# Concatenate item feature vectors for the item tower.
item_features = layers.concatenate([item_id_vec, item_country_vec, item_lang_vec], name='item_features')

# Item tower: a Dense layer to learn interactions between item features.
item_hidden = layers.Dense(32, activation='relu')(item_features)
# The output of the item tower is an item embedding vector.
item_vector = layers.Dense(32, activation=None, name='item_vector')(item_hidden)

# Compute similarity between user and item embeddings using dot product.
# This outputs a single score for each user-item pair.
dot_similarity = layers.Dot(axes=1, normalize=False)([user_vector, item_vector])

# For implicit feedback, we use a sigmoid activation to get a probability of interaction.
pred_score = layers.Activation('sigmoid', name='prediction')(dot_similarity)

# Define the full model that takes all inputs and produces the predicted interaction score.
model = Model(
    inputs=[user_id_input, user_country_input, user_lang_input,
            item_id_input, item_country_input, item_lang_input],
    outputs=pred_score
)

# Compile the model with binary crossentropy loss (for 0/1 labels) and an optimizer.
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.summary()  # Print model architecture

# Training the Model
# ====================================

# Train the model on the prepared dataset (user-item pairs with labels).
# We use a validation split to monitor performance on unseen data during training.
history = model.fit(
    [user_index_data, user_country_index_data, user_lang_index_data,
     item_index_data, item_country_index_data, item_lang_index_data],
    labels_data,
    batch_size=256,
    epochs=5,
    validation_split=0.1,
    verbose=1
)

# After training, we can inspect the training and validation performance from history.
print("Final training accuracy:", history.history['accuracy'][-1])
print("Final validation accuracy:", history.history['val_accuracy'][-1])

# Generating Recommendations (Inference)
# ====================================

# Example: Generate top-N recommendations for a specific user.
test_user_id = users_df['user_id'].iloc[0]  # pick an example user (first user in the user table)
print(f"\nGenerating recommendations for User ID {test_user_id}...")

# Prepare the user features for the model (using the same encoding as training).
user_idx = user_id_to_index[test_user_id]
user_country_idx = user_country_to_index[user_to_country[test_user_id]]
user_lang_idx = user_lang_to_index[user_to_lang[test_user_id]]

# Repeat the user feature values for all candidate items.
num_items = len(item_ids_unique)
user_idx_array       = np.full(shape=(num_items,), fill_value=user_idx, dtype='int32')
user_country_array   = np.full(shape=(num_items,), fill_value=user_country_idx, dtype='int32')
user_lang_array      = np.full(shape=(num_items,), fill_value=user_lang_idx, dtype='int32')

# Prepare item feature arrays for all items (0 to num_items-1 index for each feature).
# We can use the index mapping directly: item index 0 corresponds to item_ids_unique[0], etc.
item_idx_array       = np.arange(num_items, dtype='int32')
item_country_array   = np.array([ item_country_to_index[item_to_country[item]] for item in item_ids_unique ], dtype='int32')
item_lang_array      = np.array([ item_lang_to_index[item_to_lang[item]] for item in item_ids_unique ], dtype='int32')

# Use the model to predict scores for all item candidates for this user.
pred_scores = model.predict([user_idx_array, user_country_array, user_lang_array,
                              item_idx_array, item_country_array, item_lang_array],
                             verbose=0)
pred_scores = pred_scores.flatten()

# Rank the items by predicted score in descending order.
topN = 5
top_indices = np.argsort(-pred_scores)[:topN]  # indices of the top N scores
top_item_ids = [ item_ids_unique[i] for i in top_indices ]
top_scores = pred_scores[top_indices]

print(f"Top {topN} recommended items for user {test_user_id}:")
for rank, (item, score) in enumerate(zip(top_item_ids, top_scores), start=1):
    print(f"{rank}. Item ID {item} (predicted score={score:.4f})")

import pandas as pd
import numpy as np

# Set N = number of recommendations per user
N = 3

# Store final recommendations here
recommendations = []

for user_id in users_df['user_id']:
    try:
        # Map user features to indices
        user_idx = user_id_to_index[user_id]
        user_country_idx = user_country_to_index[user_to_country[user_id]]
        user_lang_idx = user_lang_to_index[user_to_lang[user_id]]

        # Prepare user feature arrays (same value repeated for each item)
        num_items = len(item_ids_unique)
        user_idx_array = np.full((num_items,), user_idx, dtype='int32')
        user_country_array = np.full((num_items,), user_country_idx, dtype='int32')
        user_lang_array = np.full((num_items,), user_lang_idx, dtype='int32')

        # Prepare item feature arrays
        item_idx_array = np.arange(num_items, dtype='int32')
        item_country_array = np.array([item_country_to_index[item_to_country[item]] for item in item_ids_unique], dtype='int32')
        item_lang_array = np.array([item_lang_to_index[item_to_lang[item]] for item in item_ids_unique], dtype='int32')

        # Predict scores for this user across all items
        pred_scores = model.predict([
            user_idx_array, user_country_array, user_lang_array,
            item_idx_array, item_country_array, item_lang_array
        ], verbose=0).flatten()

        # Select Top-N items by score
        top_indices = np.argsort(-pred_scores)[:N]
        top_item_ids = [item_ids_unique[i] for i in top_indices]
        top_scores = pred_scores[top_indices]

        # Store in final list
        for rank, (item_id, score) in enumerate(zip(top_item_ids, top_scores), start=1):
            recommendations.append({
                'user_id': user_id,
                'recommended_post_id': item_id,
                'score': score,
                'rank': rank
            })

    except KeyError as e:
        print(f"Skipping user {user_id} due to missing mapping: {e}")
        continue

# Convert to DataFrame
recommendations_df = pd.DataFrame(recommendations)

# Preview
print(recommendations_df.head())

recommendations_df.head(10)

# Training the Model
# ====================================

# Train the model on the prepared dataset (user-item pairs with labels).
# We use a validation split to monitor performance on unseen data during training.
history = model.fit(
    [user_index_data, user_country_index_data, user_lang_index_data,
     item_index_data, item_country_index_data, item_lang_index_data],
    labels_data,
    batch_size=256,
    epochs=5,
    validation_split=0.1,
    verbose=1
)

# After training, we can inspect the training and validation performance from history.
print("Final training accuracy:", history.history['accuracy'][-1] * 100, "%")
print("Final validation accuracy:", history.history['val_accuracy'][-1] * 100, "%")

